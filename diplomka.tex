\documentclass[11pt,,oneside]{fithesis}

\usepackage[english]{babel}
\usepackage[cp1250]{inputenc}
\usepackage[IL2]{fontenc}
\usepackage[plainpages=false, pdfpagelabels, hidelinks]{hyperref}
\usepackage{amsmath, amssymb, graphicx, amsthm, amsfonts, enumitem, algpseudocode, algorithmicx, wasysym, fixltx2e, float}

\usepackage{tikz}
\tikzset{ every node/.style={circle,fill=gray!15,draw,minimum size=1.5em,inner sep=2.5pt}}
\usepackage{array}

\usepackage[chapter]{algorithm}
\newfloat{algorithm}{t}{lop}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{definition}[thm]{Definition}

\theoremstyle{definition}
\newtheorem{example}[thm]{Example}
\newtheorem{observation}[thm]{Observation}
%\newtheorem{remark}[thm]{Remark}

\setcounter{figure}{0}
\makeatletter
\renewcommand{\thefigure}{\arabic{figure}}

\thesislang{en}
\thesistitle{Computational Complexity of Patrolling Games on Oriented Graphs}
\thesissubtitle{Diploma Thesis}
\thesisstudent{Matúš Abaffy}
\thesiswoman{false}
\thesisfaculty{fi}
\thesisyear{2014}
\thesisadvisor{doc. RNDr. Tomáš Brázdil, Ph.D.}

\begin{document}

\FrontMatter
\ThesisTitlePage

\begin{ThesisDeclaration}
\DeclarationText
\AdvisorName
\end{ThesisDeclaration}

\begin{ThesisThanks}
I would like to thank Blablabla
\end{ThesisThanks}

\begin{ThesisAbstract}
Blablabla
\end{ThesisAbstract}

\begin{ThesisKeyWords}
patrolling games, game theory, complexity, algorithm, oriented graphs, blablabla
\end{ThesisKeyWords}

\MainMatter
\tableofcontents
\pagestyle{plain}

%TODO male pismena v nazvoch/nadpisoch
\chapter{Introduction}

Blablabla

%do not forget motivation!


\chapter{Preliminaries}
% TODO review after writing the sections
In order to better understand the nature of patrolling games, we first introduce the concept of game theory.
Later in this chapter, we introduce security games, a special type of which are patrolling games. We mention some real-world applications of these theoretical models.
After that, we give an overview of patrolling games. We provide some examples and motivation why to study this field. We familiarize the reader with the basic terminology used throughout this thesis. We describe some special properties of the model of patrolling games we chose to study and we discuss the reasons why we did so.
The last section of this chapter we dedicate to the pioneers which set important directions of research in the field of patrolling games.

\section{Introduction to Game Theory}
In this chapter, we present the basics of game theory. The definitions and content used here are inspired by \cite{fudenberg}, \cite{polak} and \cite{gameTheory}.

Game theory is a mathematical study of decision making. It studies games as well-defined mathematical object.
A \emph{game} is a played between \emph{players} $P_1, \dots, P_n, n \geq 2$.
Each player $P_i$ has a non-empty set $S_i$ of \emph{pure strategies}. A strategy can be seen as an algorithm telling the player what to do throughout the game.
The outcome of the game is an element from the set
%$\prod_{i=1}^n S_i$
$\mathcal{S} = \prod_{i=1}^n S_i$
and is called a \emph{strategy profile}.
%We will denote this space of all possible strategy profiles by $\mathcal{S}$.
Each strategy profile represents some value for every player.
Therefore, every player $P_i$ has his own \emph{utility function} $U_i: \mathcal{S} \to \mathbb{R}$ which assigns \emph{payoffs} to strategy profiles.
The higher the player's payoff of a strategy profile, the better the profile for the player.
The above representation of a game is called normal form. The normal game is usually represented by a matrix and is mostly used when a strategy represents a single action.

The extensive form of a game is appropriate to formalize games with time sequencing of \emph{moves}.
Here, games are played on trees, where each node with its outgoing edges represents a point of choice for a player.
Examples of both normal- and extensive-form games can be found in \cite{gameTheory}.

A \emph{zero-sum} game is a game such that $\sum_{i=1}^n u_i(s) = 0$ for all $s \in \mathcal{S}$.
In a two-player zero-sum game, one player's gain is equal to his opponent's loss.
Non-zero sum games are reffered to as \emph{general-sum} games.

Of great importance is the concept of \emph{mixed strategies}.
A pure strategy is a strategy which determines the move a player will make for every possible situation that could occur.
A mixed strategy is a probability distribution
% odkaz na probability distribution?
over pure strategies.
Using a mixed strategy allows a player to randomize.
This means a player can make different moves under the same game conditions.
As each pure strategy is also a mixed strategy, we will refer to mixed strategies just as \emph{strategies}.

A player's \emph{best response} is the strategy (or strategies) which yields him the greatest payoff given the other players' strategies.

When it comes to solving the game, some form of equilibrium strategy profile is in demand. The best known solution concept in game theory is \emph{Nash equilibrium} (NE) \cite{nash}, in which no player can gain anything by changing only his strategy. That is, each player's strategy is a best response to the other players' strategies.

% zmazat?
Different solution concepts were designed for specific game models. We are specially interested in Stackelberg model, which we describe in the next section.

\section{Security Games}

Security games are two-player games in which \emph{defender} allocates resources to guard targets against an \emph{attacker} (also called \emph{adversary} or \emph{intruder}), who aims to attack some target.\footnote{In the rest of the paper, we denote the defender with "she" and the attacker with "he".}
Players' utilities are determined by the attacked target and the position of defender's resources during the attack.
If the target is defended (attack is unsuccessful), defender's utility increases and attacker's utility decreases.
Although one player's gain means other player's loss, security games are non-zero sum games in general.

A simple example of a security game is a problem of protecting $n$ banks in a city by $k, k < n$ policemen. Attacker wants to rob some bank and his attack is successful if and only if he attacks a bank without a policeman. The defender's goal is to distribute the security forces in a way that minimizes the probability of a successful attack.

In security games, it is mostly assumed that the attacker can observe the defender's moves and learn the defender's strategy before attacking.\footnote{In many situations attacker's perfect knowledge about the defender's strategy is at least questionable. Security games in which the attacker has only limited knowledge about the defender's strategy are studied e.g. in \cite{limited1, limited2, limited3, limited4}.}
This leads to adopting the \emph{Stackelberg} model.
In a Stackelberg game, the \emph{leader} (defender) first commits to a strategy, and then the \emph{follower} (attacker) chooses when and where to attack.
The standard solution concept for Stackelberg games is \emph{strong Stackelberg equilibrium} (SSE, also called \emph{leader-follower equilibrium}), which gives the leader the maximum expected utility in the worst-case scenario.
A crucial assumption is that the follower has a perfect knowledge of the defender's strategy and plays a best response to maximize his expected utility.
One of the advantages of an SSE is that being a Stackelberg leader cannot hurt the defender \cite{convex, nashStack}. Furthermore, under certain natural assumptions about security games, any SSE is also an NE \cite{interchangability}.

\subsection{Secuirty Games in Real World}

The leader-follower Stackelberg game model is actually utilized in many real-world security situations.
We are aware of five applications based on this model which were already put into use in the United States. We present them in chronological order as they were deployed.

The first deployed decision-support application is the \emph{ARMOR} (Assistant for Randomized Monitoring over
Routes) security system \cite{armor}.
It has been deployed since August 2007 at the Los Angeles International Airport (LAX).
It is used for randomizing allocation of checkpoints on the roadways and assigning canine patrol routes within the terminals.

The second application, deployed by Federal Air Marshals Service (FAMS), is called \emph{IRIS} (Intelligent Randomization In Scheduling) \cite{iris}.
IRIS helps to create a flight coverage schedule for armed marshals who can defeat terrorist attacks.
Scheduling constraints in IRIS are much more complex than in ARMOR as the marshals are limited by their location and timing constraints.
Although IRIS was developed for FAMS, it provides a general framework for solving patrolling scheduling problems in other transportation networks as well, e.g. in New York City subway system.

Another security application using the Stackelberg game model is the \emph{GUARDS} (Game theoretic Unpredictable and Randomly Deployed Security) system \cite{guards}.
It helps United States Transportation Security Administration (TSA) in resource allocation tasks for protection of more than 400 airports.
In constrast with ARMOR and IRIS, which focused on one security activity, GUARDS reasons about hundreds of heterogenous defender activities, large number of diverse potential threads, and hundreds of potential end-users.

Other example of a game-theoretic security system is \emph{PROTECT} (Port Resilience Operational/Tactical Enforcement to Combat Terrorism) \cite{protect}.
It has been currently deployed by the United States Coast Guard (USCG) in the ports of Boston, New York and Los Angeles \cite{protect_coastguard}.
In contrast to the three beforementioned security applications, PROTECT relies on a quantal responce (QR) \cite{quantal} model of the adversary's behavior, i.e. the attacker has only bounded rationality.
It was the first time that the QR model has been used in a real-world security application \cite{protect}.

The last real-world security application we mention is \emph{TRUSTS} (Tactical Randomization for Urban Security in Transit Systems) \cite{trusts}, used by the Los Angeles County Sheriff's Department (LASD).
The main purpose of TRUSTS is fare-evasion deterrence in urban proof-of-payment transit systems.
In these systems, passengers are legally, but not physically, required to purchase tickets.
Patrol units move about the system and fine fare-evading passangers.
The optimization objective is to maximize the total revenue (ticket sales plus fines) to the government.
In \cite{trusts2}, the authors extend the TRUSTS system to incorporate also crime prevention and counter-terrorism.

\section{Patrolling Games}
\label{sec:PG}

In this section, we introduce the concept of patrolling games in general and we give a simplified motivational example of what it might look like in real life.
Later, we informally describe the model adopted by us and we discuss its advantages.

Patrolling games are a special type of security games.
Their unique feature is that the defender's resources can change their position in time.
Furthermore, the attacker needs some finite time, called \emph{penetration time} (also \emph{attack length}), to succesfully complete an attack on a target.
During this time, the defender can detect the intrusion in which case the attack is unsuccessful.
The defender's goal is to commit to a strategy which maximizes the probability of detecting an intrusion.
The attacker's goal is to maximize the probability of a successful attack.
The resources controlled by the defender are called \emph{patrollers} (also \emph{robots}).
As the defender is represented by her resources, we use these two terms interchangably.
That is, under "defender" we might mean her resources and vice versa.

We illustrate the problem of patrolling on a simple example mentioned in \cite{miso}.
Let us assume a city with $n$ banks which the police can protect against robbery.
The police have $k, k < n$ police officers with cars available.
We assume that banks cannot let the police know that an attack is under way.
The goal of the police is to create a surveillance plan which minimizes the possilibity of a successful attack.

\

There are many models of patrolling games which differentiate from each other in many ways (see Section \ref{sec:patrolLiterature}).
We work with a simple model of patrolling games which was introduced in \cite{miso}.
The results achieved in this thesis can be easily extended to more general models, which is explained later.
%TODO link where the explanation is

We model a patrolling game as a two-player zero-sum Stackelberg security game in extensive form with infinite horizon.
The game is played on a directed graph, which represents the possible locations and moves of the defender.
Some of the nodes of the graph are targets, all of which are equivalent:
the attacker and the defender have no preferences over the targets and the attacker needs $d \geq 1$ time units to successfully attack any target.
The defender starts in some initial target node and moves from node to node.
Every move, consisting of the defender traversing an arbitrary edge, takes one time unit (step).
The defender chooses the next node randomly and based on the history of all already visited nodes.

The assumed Stackelberg model implies a strong adversary, i.e. the attacker has complete knowledge of the defender's strategy and her current position.
As the attacker observers the defender, he may either wait, or attack some target.
When he attacks a target, he has to stay there for the next $d$ steps, i.e. he cannot leave even if he sees the defender approaching.
Realize that a strong adversary would never attack a target currently visited by the defender.
However, he can attack this target right after the defender's departure, long before the attacker arrives to the next node.
That leads us to the following definition of a detected attack---if the defender is in node $v$ and the attacker attacks a target $u$, the attack is detected and the attacker is captured if and only if the defender visits $u$ in the next $1, \dots, d$ steps, even if $u = v$ \cite{iti1}.
Otherwise, the attack is successfully completed.
We assume the game ends after the attack is detected or completed, i.e. no more than one attack is possible.
The defender aims to minimize the probability of a successful attack, the aim of the attacker is the opposite.

The Stackelberg setting enables the attacker to play the best response to the defender's strategy.
Therefore, the attacker cannot increase his utility using randomization, so we consider only pure attacker's strategies.
Note that we model a patrolling game as a zero-sum game, i.e. the higher the attacker's utility, the lower the defender's utility.
This means the attacker's best response represents the worst case for the defender.
In other words, the attacker tries to hurt the defender as much as possible.
Bear in mind that when finding the optimal strategy for the defender, we are concerned with the worst case scenario.

For now, we assume that the defender has only one patrolling unit as we draw on the results achieved in \cite{miso, iti1}.
We generalize this model by allowing multiple patrolling units in Chapter \ref{chap:multiPatrol}.

From now on we also assume a single adversary who can attack only with a single resource.
This assumption is adopted by all of the works mentioned in Section \ref{sec:patrolLiterature} as well.

%TODO odkaz na formalnu definiciu

The reasons for choosing this model are described in detail in \cite{miso}.
The basic motive is to consider a model as simple as possible, yet still reasonable enough.
This motive resulted in opting of the following parameters of our model: graph structure, discrete time, no preference over targets, unified attack length, immobile targets, and only one attack possible.

It turns out that the assumption of all edges being of length 1 is quite sufficient. Indeed, the case where edges can have different natural lengths can be reduced to our simple model by splitting each edge of length $t$ to $t$ sequentially connected edges of length one. A more general case with rational lengths of edges can be reduced to the previous case by multiplying both the lengths of edges and the attack length by a suitable number.

%TODO sensing capabilities generalization

The motives behind choosing the Stackelberg model are not restricted to simplicity.
Besides the reasons mentioned in \cite{miso}, the Stackelberg setting together with the game being zero sum lead to one favorable characteristic---the optimal defender's strategy is optimal also with respect to the worst-case scenario.

The motive for absolute optimality led to allowing the defender to make her movement choice based on all the moves she already did.
This is in contrast with all the other patrolling literature we are aware of, where the defender can decide where to move based only on the last $h$ visited nodes.
Strategies with $h = 1$ are based only on the current defender's location and we call them \emph{stationary} (or \emph{Markovian}).
In \cite{arbitrary} there is an example where no optimal defender's strategy is stationary.
In \cite{iti2} it has been shown that, in general, there might be no optimal defender's strategy which depends only on the last $h$ visited nodes for any $h \in \mathbb{N}$.

%TODO nejake odkazy do dalsich sekcii na examples a pod.

%\section{Related Work}
\section{Basic Literature on Patrolling Games}
\label{sec:patrolLiterature}
In this section, we give an overview of the research in the field of the patrolling games in the last years. We address several papers and models. For each paper, we state the differences from our model and we mention the most significant results achieved.

In the rest of this section, we use the following assumptions which hold true unless stated otherwise.
A patrolling game is played on a directed graph with distances between any two neighbour nodes being the same.
We assume a Stackelberg game model, i.e. the attacker knows the defender's strategy and the current position of the patrolling unit(s).
And to intercept an attack, the defender has to come to the node where the attack is taking place.

\subsection{Perimeter Patrol}
The first reference in almost every robot patrolling paper is \cite{perimeter}. The authors consider a problem of multi-robot patrol around a closed area with the existence of a single adversary trying to enter the area. A real life motivation might be the following scenario. An intruder tries to cross a border while some patrolling units aim to detect this intrusion.

The authors show that in adversial settings non-deterministic defender strategies are needed. They demonstrate it on an example similar to the following situation. A robot moving with velocity ${1m/s}$ is required to patrol around a fence of length 25 meters. Assume that an adversary needs 20 seconds to penetrate the area through the fence. If the robot moves deterministically, in any point of time a fence section of lenght of at least 5 meters will not be visited by the robot in the next 20 seconds. The intruder can guarantee penetration by entering through this section. However, if the robot moves in a non-deterministic way, it can commit to a strategy that does not allow the adversary to penetrate the area with probability 1.

In the paper, a circular graph is considered. All nodes of the graph are targets with unified attack length. A system consisting of $k$ mobile robots is considered. Robots move simultaneously. The model is Markovian, where for each node the robot goes to the right with probability $p$ and to the left with probability ${1 - p}$.\footnote{Two other movement models, where the robot has directionality associated with its movement, are considered. The achieved results, such as polynomial time algorithm for finding the optimal strategy, apply for these models too.} The authors prove that to maximize the probability of penetration detection (\textsf{ppd}), the robots have to be placed uniformly along the cycle and they have to be coordinated, i.e. all robots move together in the same direction. This reduces the problem to a much simpler instance with smaller graph and only one robot. The main result is a polynomial time algorithm which computes the optimal defender's strategy.

Authors' subsequent papers concerning multi-robot perimiter patrol in adversial settings include \cite{perimeter2} and \cite{perimeter3}. In \cite{perimeter2} it is shown that in the case the adversary has no knowledge of the patrol scheme, a deterministic algorithm is optimal. However, the deterministic algorithm creates high deviations between the  \textsf{ppd} values in the nodes and it is easy to detect. Therefore, the authors present an algorithm that deals with adversary having partial information. This algorithm maximizes the expected \textsf{ppd} along with minimizing the deviation between the \textsf{ppd} values throughout the perimeter. Its performance was verified by experiments with human subjects.

\subsection{Arbitrary Graph Topologies}

A more general graph topology is addressed in \cite{arbitrary}. The authors consider a model with a single robot patrolling on an arbitrary directed graph. Not all nodes of the graph are necessarily targets and each target node $i$ requires the intruder ${d_i}$ steps to enter.
The robot is equipped with a sensor able to detect the intruder. The robot's sensing capabilities are represented by a matrix ${V(n \times n)}$ where ${v_{i,j} = 1}$ iff node $j$ can be sensed from node $i$.
% footnote?
The patroller's sensing capabilities in our model are equivalent to the case of $V$ being an identity matrix.

The possible outcomes of the game are defined as follows:
\emph{intruder-capture}, when the intruder attacks some node $i$ and the robot senses $i$ in the next ${d_i}$ steps;
\emph{penetration-i}, when the intruder attacks node $i$ and the robot does not sense $i$ in the next ${d_i}$ steps;
\emph{no-attack}, when the intruder never attacks.
For each possible outcome, patroller's and intruder's payoffs are defined.\footnote{The game is a general-sum game with some consistency constraints over the players' payoffs.} Explicitly, these values are known to both players. However, the model can capture the case when the intruder's preferences over the targets are uncertain to the patroller.

The authors cope with the presence of the infinite horizon in the game by introducing symmetries. They allow the patroller to choose the next action based only on the last ${|H|}$ actions, where ${|H|}$ is finite and constant during the whole game. Higher value of ${|H|}$ leads to possibly higher expected utility of the patroller, but also to higher computational complexity for finding the optimal patrolling strategy. The authors provide a multi-bilinear mathematical programming formulation to find the optimal patroller's strategy when ${|H| = 1}$. They state that formulations with ${|H| > 1}$ can be obtained by extending this one.

Further, the paper is concerned with heuristics for more efficient problem solving. In realistic settings the number of targets is significantly smaller than the number of all nodes. That affords opportunity to reduce the programming problems' searching space, the number of variables and constraints.
% use dash instead of comma? "- the number of ..."
First, the authors are interested in a deterministic strategy which detects any attack with probability 1. To solve the problem of finding such a strategy, they formulate an integer linear program.
Second, the authors aim to minimize the number of nodes over which the patroller will randomize when using an optimal strategy. They formulate another integer linear program for that.
Third, they introduce the concept of action dominance for the intruder.
For an example setting, the reduction of nodes and the elimination of dominated actions resulted in the following improvement. Both the number of bilinear problems to be solved and the average computational time for solving a single problem decreased from ${29^2}$ to 18 and from more than 30 minutes to less than 5 minutes, respectively.

The rest of the paper discusses the problem of finding an upper bound for ${|H|}$, say ${|\overline{H}|}$, such that there is an optimal strategy, among all the strategies,
% - among all the strategies -
which uses no more than length ${|\overline{H}|}$ of the history, i.e. for any ${|H| > |\overline{H}|}$ the patroller's expected utility keeps constant. Throughout the paper it is claimed that this treshold exists. However, in \cite{iti2} this claim has been proven \emph{incorrect}. Moreover, in \cite{arbitrary} they formulate a theorem that for a fully connected topology, ${|\overline{H}| = 0}$. That is also \emph{incorrect}. The proof sketch is done on a complete graph with 3 nodes, each with attack length 2. The authors prove that their algorithm gives the same expected utility when ${|H| = 0}$ and ${|H| = 1}$. Generalization of any kind is omitted. The highest expected utility when ${|H| = 0}$ is ${5/9}$. However, in \cite{iti1} there is a strategy with ${|H| = 2}$ giving expected utility ${(\sqrt{5} - 1)/2}$, which is strictly greater than ${5/9}$.

The authors extend the model from \cite{arbitrary} to more realistic settings in \cite{powerlessIntruder}. They weaken the intruder in two ways. Their first extension introduces the intruder's movement. The intruder does not appear directly at the target, rather he has to traverse the graph along paths connecting \emph{access areas} to targets. The intruder can move infinitely fast. Covering a path takes only one step during which the intruder can be detected on every node of the path. In their second extension, some cells are hidden to the intruder. As he cannot observe the whole environment, he cannot perfectly know the position of the patroller when attacking. A concept of a \emph{state} is introduced, representing when (how many steps ago) and where (in which node) has the patroller been observed for the last time. For both extensions the authors provide mathematical programming formulations, reduction algorithms and experimental evaluations.

\subsection{Multiple Patrolling Units}

The authors of \cite{arbitrary} generalized their model by introducing multiple patrolling robots in \cite{multi-robot}. On the other hand, they simplified the model in many ways. They assume that the robots' strategy is Markovian, that the robots and the intruder have the same preference ordering over the targets and that the robots can sense only the nodes they are in.

%nejake odsadenie?
%podnadpis Smallest Number of Robots
The first problem addressed in the paper is finding the smallest number of robots needed to capture any attack with probability greater than zero. With a smaller number of robots the intruder could successfully attack with probability 1. The authors show that this problem is equivalent to finding the smallest number of maximal cliques such that each target belongs to at least one clique. A simplified idea behind this is the following.
% "the following:" - throughout the document

Let us assume a setting with a graph $G$ and a set of targets $T$. Let $T' \subseteq T$ be a subset of targets. If there is a subgraph $G' \subseteq G$
%such that $T'$ is a subset of nodes of $G'$
%such that for each $t \in T'$ it holds that
%$t$ is a node of $G'$
such that for each $t \in T'$ and for each node $v$ from $G'$ there is a path from $v$ to $t$ in $G'$ of length at most $d_t$, then a single robot can patrol all the targets $T'$. In other words, the robot has a strategy such that in each node of $G'$, for all $t \in T'$ the robot will visit $t$ in at most $d_t$ steps with probability strictly greater than zero.
The goal is to find the smallest set of such subgraphs that would cover all the targets.

The authors build an abstraction $Q$ of the original graph $G$. They formulate an algorithm to find the set of all maximal \emph{labeled cliques} in $Q$
%these cliques represent the beforementioned subgraphs
and a linear program to find the smallest labeled clique cover. The result of the program is the smallest number of robots needed to patrol the environment.

%nejake odsadenie?
%podnadpis Coordination between Robots
The second problem studied in the paper regards the coordination between the robots. The authors consider two coordination dimensions,
%: in computing the patrolling strategy and in partitioning the environment.
each with three possible values.

The first dimension refers to the degree of \emph{coordination in computing the patrolling strategy}, which can be of one of the following types: 

\emph{Joint strategy}: the strategy of each robot depends on its position and on the position of all the other robots.
One strategy regarding the movement of all the robots is computed centrally. The solution contains $O(n^{|R|})$ probability variables, where $n$ is the number of nodes and $|R|$ is the number of robots.

\emph{Disjointed strategies}: the strategy of each robot depends only on its position. The strategy of each robot is computed centrally keeping into account the strategy of all the other robots. The solution contains $O(|R|\cdot n^2)$ probability variables.

\emph{Separated strategies}: each single robot computes its own strategy without considering the strategies or positions of the others. This reduces the multi-robot patrolling problem into a set of $|R|$ single-robot patrolling problems, each with $O(n^2)$ probability variables.

The second dimension refers to the degree of \emph{coordination in partitioning the environment}. Each robot is constrained to move within a portion of $G$ assigned to it. Possible levels of this coordination are:

\emph{Full assignment}: each robot can potentially move in every vertex of $G$.

\emph{Maximal clique assignment}: each robot is assinged a maximal labeled clique (in the abstraction $Q$ of $G$). If the cliques overlap, some targets may be patrolled by multiple robots.

\emph{Separated assignment}: as the previous case, but the cliques assigned to the robots may not overlap (they do not have to be maximal cliques). Therefore, each target is patrolled by only one robot.

In the next part of the paper, the authors combine the two mentioned coordination dimensions together. They show that only some combinations are reasonable. For example, joint strategy with separated assignment can be reduced to separated strategy with separated assignment. That is because robots patrolling separated portions of $G$ do not need to cooperate. A mathematical programming formulation is proposed for each reasonable combination of values.

Experimental results show that the higher the degree of coordination, the higher the computational effort and the robots' utilities.\footnote{There is an exception to this rule in the case of separated strategies: separated assignment turned out to be more time-consuming and gave higher utilities than maximal clique assignment. This shows that overlapping patrolling areas without any strategy coordination have negative effect on the expected utility.}
An easy observation was that joint strategies do not scale with large settings (more than 15 nodes) as they require much memory.
Special attention was paid to two coordination combinations: disjointed strategy with maximal clique assignment ($\mathcal{D}_3$) and separated strategy with separated assignment ($\mathcal{D}_5$). $\mathcal{D}_5$ consumed less time than $\mathcal{D}_3$ just because the number of possible assignments was small. With larger settings, the number of all the possible separated assignments increases drastically faster than the number of maximal cliques. Another interesting observation is that higher number of robots can lead to lower computational time. That is because adding more robots decreases the portion of $G$ assigned to each robot and the computational time strongly depends on the size of this portion. For more conclusions, see \cite{multi-robot}.

\subsection{Moving targets}

So far, the targets carrying value were static. In \cite{movingTargets}, the authors assume that these valuable targets can change their positions in time. This is motivated by real-world scenarios such as protection of vessels transiting waters with high pirate activity and unmanned aerial vehicle-based surveillance protecting moving ground targets.

The model considered in \cite{movingTargets} is based on the model used in \cite{arbitrary}. It assumes unified attack length and first-order Markovian defender's strategy.
% and identity sensing capabilities matrix.
In addition, targets are allowed to move through the area according to a deterministic movement schedule. This schedule is a fixed property of the environment and is known to both players.

The game is played in $|T|$ turns, thereafter it is repeated \--- i.e. the targets move in cycles. In each turn, the defender can move to an adjacent node and the targets can move to any node. The movement of the targets is defined by an arbitrary function ${f : Q \times T \to V}$, where $Q$ is the set of targets, $T$ is the set of turns and $V$ is the set of nodes.
%vynechat formalizmy a skratit to? "In contrast with the defender, the targets can move to any node, not just the adjacent one."
Note that targets are not nodes anymore, but they are positioned in nodes.
% rather???
To cope with the target movement, the defender's strategy is time-dependent. The defender chooses her next action based not only on her current node but also on the current turn.

The authors formulate a multi-bilinear program to find the optimal defender's strategy.
They experimentally compare the performance of the stationary and time-dependent strategies. They do so in terms of the reached game value and computation time. Evaluation examples have different types of graph structure (grid with holes vs. full grid) and different types of target movement (distance between targets changing or not changing). 
% dat prec zatvorky a ich obsah

The results of the experiment show that in the game with mobile targets time-dependent strategies lead to significantly higher utility value. In the experimental results presented in the paper, time-dependent strategies preceded the stationary ones by approximately 50--300\%. However, the computational time was 10--1000 times larger. The authors concluded that time-dependent strategies are reasonable to be used.

\

A noticeably novel game model called MRMT\textsubscript{sg} (multiple Mobile Resources protecting Moving Targets) was introduced in \cite{line}. In MRMT\textsubscript{sg}, the defender's strategy space is descretized and attacker's strategy space is continuous. A major motivation in the paper is the problem of protecting ferries with fast patrol boats.

The authors consider $L$ targets moving along a one-dimensional domain, specifically a straight line between two points $A$ and $B$. The targets have fixed daily schedules known to both players. The targets move in time continuously between points $A$ and $B$.
%... move continuously ... in time. ???
The defender has $W$ mobile patrollers that can move between $A$ and $B$. All patrollers have a common maximum speed $v_m$ and a protection radius $r_e$. A patroller is \emph{protecting} a target if the distance between them is no more than $r_e$. The attacker chooses a certain time and a certain target to attack. The probability of attack success depends on the positions of the patrollers at that time, i.e. the attack is instant and there is no attack length. This probability is a decreasing function of the number of patrollers protecting the target. An attack on a target can be successful even if there is a patroller protecting the target. The case when an attack on a target is successful iff the target is not protected by any patroller is just a special case.

The authors provide an efficient linear program called CASS (Solver for Continuous Attacker Strategies) to solve  MRMT\textsubscript{sg}. The effiecency of CASS consists in the represention of the defender's strategies as marginal probability variables and in using \emph{sub-interval analysis} to model the attacker's continuous strategy space.


% \subsection{Other}
% modularity
% alarms
% euclidian??

%TODO - tabulka!


\chapter{Preliminaries}

The definitions we use herein are from \cite{iti1, iti2}. Although we use the model from \cite{iti1}\footnote{In \cite{iti1}, in contrast to \cite{iti2}, not all nodes are necessarily targets and the attack length is unified.}, we prefer some of the novel syntax introduced in \cite{iti2}.

\section{Basic Definitions}
\label{sec:definitions}

We use $\mathbb{N}$ to denote the set of positive integers.
% set of non-negative??? do we need it?
The sets of all finite and infinite words over an alphabet $\Gamma$ are denoted by $\Gamma^*$ and $\Gamma^{\omega}$, respectively.
The empty word is denoted by $\epsilon$.
The length of a given word $w \in \Gamma^* \cup \Gamma^{\omega}$ is denoted by $|w|$, the length of an infinite word is $\infty$.
We write $\Gamma^{\leq k}$ for the set of all words $w \in \Gamma^*$ satisfying $|w| \leq k$.
The individual letters of a non-empty word $w \in \Gamma^* \cup \Gamma^{\omega}$ are denoted by $w_0w_1\dots$.
We write $last(w)$ to denote the last letter of a finite non-empty word $w$.
Given two words $w, w' \in \Gamma^* \cup \Gamma^{\omega}$ we write $w \preceq w'$ iff $w$ is a prefix\footnote{An infinite word is a prefix of another word if and only if they are the same.} of $w'$, i.e. iff there exists a word $w'' \in \Gamma^* \cup \Gamma^{\omega}$ such that $w' = ww''$.
%TODO do we need the $w$ to be possiblyt infinite?
Furthermore, we write $w \prec w'$ iff $w \preceq w'$ and $w \neq w'$.

Given an at most countable set $S$, a \emph{probability distribution} over $S$ is a function $\delta : S \to [0,1]$
such that $\sum_{s \in supp(\delta)} \delta(s) = 1$, where the set $supp(\delta) = \{s \in S \mid \delta(s) \neq 0\}$ is the \emph{support} of $\delta$.
We denote the set of all probability distributions over $S$ by $\Delta(S)$.

%A \emph{directed graph} is an ordered pair $G = (U, E)$, where $N$ is a finite set of \emph{nodes} and $E \subseteq U \times U$ is a set of \emph{edges}.

\begin{definition}
A \emph{patrolling game} is a tuple $\mathcal{G} = (U, E, T, \hat{u}, d)$ where $U$ is a finite set of \emph{nodes}, $E \subseteq U \times U$ is a set of \emph{edges}, $T \subseteq U$ is a set of \emph{target nodes}, $\hat{u} \in T$ is the \emph{initial node} and $d$ is the \emph{attack length}.
\end{definition}

The pair $(U, E)$ is the directed graph on which the game $\mathcal{G}$ is played. 
Given a node $u \in U$, we define $succ(u) := \{v \in U \mid (u, v) \in E\}$ to be the set of all \emph{successors} of $u$.
A \emph{path} is a finite or infinite word $w \in U^* \cup U^{\omega}$ such that $w_{i+1}$ is a successor of $w_i$ for all $0 \leq i < |w|$.
A \emph{history} is a finite non-empty
%ALLOWING ALSO EMPTY if no initial node
path whilst a \emph{run} is an infinite path.
We denote the set of all histories by $\mathcal{H}$ and the set of all runs by $\mathcal{R}$.
Given a set of histories $H \subseteq \mathcal{H}$, we use $\mathcal{R}(H)$ to denote the set of all runs $\omega \in \mathcal{R}$ such that $h \preceq \omega$ for some history $h \in H$
%footnote?
(we write $\mathcal{R}(h)$ instead of $\mathcal{R}(\{h\})$).

\begin{definition}
A \emph{defender's strategy} is a function $\sigma : \mathcal{H} \to \Delta(U)$ such that $supp(\sigma(h)) \subseteq succ(last(h))$ for every $h \in \mathcal{H}$.
The set of all defender's strategies is denoted by $\Sigma$.

An \emph{attacker's strategy} is a function $\pi : \mathcal{H} \to T \cup \{\perp\}$ such that whenever $\pi(h) \neq$ $\perp$, then for all $h' \prec h$ we have that $\pi(h') =$ $\perp$.
The set of all attacker's strategies is denoted by $\Pi$.
\end{definition}

Let us stop for a while and intuitively explain the meaning of players' strategies.
In any part of the game, the actual history describes the order of the nodes which were already visited by the defender.
After following a history $h$, the defender randomly chooses the next node according to the distribution $\sigma(h)$. At the same time, the attacker chooses whether to attack a target $t$ ($\pi(h) = t$), or wait ($\pi(h) =$ $\perp$).
Note that the attacker can attack only once during a play.
Also note that $\Pi$ contains only pure strategies, i.e. the attacker cannot randomize.
This is because the attacker cannot hurt the defender by randomizing, as is discussed in Section \ref{sec:PG}.

Note that for a given defender's strategy $\sigma$, some histories cannot happen.
For a given defender's strategy $\sigma$, we define the set $\mathcal{H}(\sigma) \subseteq \mathcal{H}$ of \emph{relevant} histories, consisting of all $h \in \mathcal{H}$ starting in $\hat{u}$ such that if $h'u \preceq h$ for some $h' \in \mathcal{H}$, $u \in U$, then $\sigma(h')(u) > 0$.
Clearly, only relevant histories can happen with positive probability.
In the future, we may talk about a defender's strategy $\sigma$ and not define $\sigma(h)$ for irrelevant histories $h$.

%nejake keci okolo probability space?
Also note that a given defender's strategy $\sigma$ determines a unique probability space over all infinite paths initiated in a given $u \in U$ (see \cite{chung}), and we denote the associated probability measure by $\mathcal{P}_u^{\sigma}$.

Given an attacker's strategy $\pi$, we say that a run $\omega$ \emph{contains a successful attack} if there exist a
%finite prefix!!! fml! -> history $h \preceq \omega$ ?
finite prefix $h$ of $\omega$ and a target $t \in T$ such that $\pi(h) = t$ and $t$ is not visited in the first $d$ steps after the history $h$.
We say that a run is \emph{defended}, if it does not contain a successful attack.
For every node $u \in U$, we use $\mathcal{D}_u[\pi]$ to denote the set of all defended runs initiated in $u$.
Hence, $\mathcal{P}_u^{\sigma}(\mathcal{D}_u[\pi])$ is the probability of all defended runs initiated in $u$ when the defender and the attacker use strategies $\sigma$ and $\pi$, respectively. We omit the subscript $u$ in $\mathcal{P}_u^{\sigma}$ and $\mathcal{D}_u[\pi]$ when $u = \hat{u}$.

%TODO - what precision in definition do we really need? (optimality in/Stackelberg value of $u$ - srsly?)
\begin{definition}
For all $u \in U$ and $\sigma \in \Sigma$, the \emph{value of $\sigma$} is defined as $val_u(\sigma) = \inf_{\pi \in \Pi}\mathcal{P}_u^{\sigma}(\mathcal{D}_u[\pi])$.
The \emph{Stackelberg value} of $u$ is defined as $val_u = \sup_{\sigma \in \Sigma}val_u(\sigma)$.
The (Stackelberg) value of the game $\mathcal{G}$ is defined as $val = val_{\hat{u}}$.

Let $\epsilon \geq 0$. A defender's strategy $\sigma^*$ is called \emph{$\epsilon$-optimal} if $val(\sigma^*) \geq val - \epsilon$.
A $0$-optimal strategy is called just \emph{optimal}.
\end{definition}

%TODO preformulovat, prip. presunut inam
At many places we consider strategies obtained by “forgetting” some initial prefix of the history. Formally,
for all $h \in \mathcal{H}$ and $\theta \in \Sigma \cup \Pi$, we define a strategy $\theta_h$ by $\theta_h(h') = \theta(hh')$.

\section{Examples}

In this section, we provide a few examples to better understand the nature of patrolling games. Each example illustrates certain characteristic of our model and serves as an explanation for why we have chosen this model.
Examples \ref{ex:stationary} and \ref{ex:initial} are from \cite{miso}, the setting in Example \ref{ex:triangle} is from \cite{iti1}.

\begin{figure}[h]

\begin{tabular}{ m{4.2cm} m{5.3cm} m{3cm} }

\begin{tikzpicture}
  \foreach \pos/\name in {{(0,0)/a}, {(1.25,0)/b}, {(2.5,0)/c}}
        \node (\name) at \pos {$\name$};
   \foreach \from/\to in {a/b, b/c}
	\draw [<->] (\from) -- (\to);
\end{tikzpicture}

&

\begin{tikzpicture}
  \foreach \pos/\name in {{(0,0)/k}, {(1.25,0)/l}, {(2.5,0)/m}, {(3.75,0)/n}}
        \node (\name) at \pos {$\name$};
   \foreach \from/\to in {k/l, l/m, m/n}
	\draw [<->] (\from) -- (\to);
\end{tikzpicture}

&

\begin{tikzpicture}
%[scale=1, transform shape]
  \foreach \pos/\name in {{(0,0)/s}, {+(0: 1.5)/u}, {+(60: 1.5)/v}}
        \node (\name) at \pos {$\name$};
   \foreach \from/\to in {s/u, u/v, v/s}
	\draw [<->] (\from) -- (\to);
\end{tikzpicture}

\end{tabular}

\vspace{-5pt}
\caption{{Simple graphs on which our example patrolling games are played}}
\label{fig:examples}
\end{figure}

\vspace{-5pt}
\begin{example}
\label{ex:stationary}
Consider a patrolling game played on the left graph from Fig.~\ref{fig:examples}, where all nodes are targets, $a$ is the initial node and $d = 4$.
The defender has to move to the node $b$ from both nodes $a$ and $c$.
All that the defender has under control is what she does in the node $b$.
If we restrict the defender's strategies to be stationary, the best value the defender can achieve is 3/4, when she goes from $b$ to $a$ with the same probability as from $b$ to $c$, i.e. with probability 1/2.
However, the defender can protect all the targets with probability 1 with a simple deterministic strategy $\sigma^*$ resulting in a run $(abcb)^\omega$.
Note that $\sigma^*$ depends only on the last two nodes---when the defender is in the node $b$, she moves to the node she has not visited in the last two steps.
\end{example}

\begin{example}
\label{ex:initial}
Now consider a patrolling game played on the middle graph from Fig.~\ref{fig:examples}, where $T = {m, n}$ and $d = 2$ and let us suppose that any node can be the initial node, not just the targets.
The value of the game clearly depends on the initial node.
If $k$ is the initial node, the attacker attacks $n$ as soon as possible and the defender has no chance to intercept this attack, i.e. $val_k = 0$.
Otherwise, the defender can defend both targets with probability 1.

Considering a general game, an interesting question arises: for which node(s) $u \in U$ does it hold that $val_u =$ max$_{v \in U} val_v$?
We show that, in general, all target nodes satisfy this condition, i.e. no initial node would yield higher value of a game than any initial target node.
This is a corollary of Lemma \ref{lem:value} and the reason why we enforce the initial node to be a target.
\end{example}

\begin{example}
\label{ex:triangle}
Consider a patrolling game played on the right graph (triangle) from Fig.~\ref{fig:examples}, where all nodes are targets, $s$ is the initial node and $d = 2$.
This example is rather sofisticated than the previous two, because the defender cannot protect all the targets with probability 1.
A simple defender's strategy that comes to one's mind is to choose the next node uniformly, i.e. with probability 1/2.
The value of this strategy is 1/2---attacker's best response is to attack a target when the defender leaves it.
One can show that this strategy is optimal among all stationary strategies.
%TODO - ref to proof
However, consider a strategy $\sigma^*$ which chooses the previously visited node with probability $(\sqrt{5} - 1)/2$ and the other node with the remaining probability $(3-\sqrt{5})/2$.
In the first step, when there is no previously visited node, $\sigma^*$ chooses the next node uniformly.
It is easy to show that $val(\sigma^* = (\sqrt{5} - 1)/2$.
In \cite{iti1}, it has been shown that $val(\sigma^*) = val$ and that all optimal strategies are the same as $\sigma^*$ except for the choice in the first step.
%TODO - own proof?
This means that irrational numbers may inevitably occur in both the probability distribution of the defender's strategy and the value of the game.

The last thing we want to demonstrate on this example is that if the defender had the information whether the attacker has already attacked or not, she could commit to a strategy resulting in a higher value.
Indeed, assume that the defender chooses the next node uniformly until she gets the information about an initiated attack.
At that moment, she goes to the previously visited node with probability 2/3 and to the other node with probability 1/3.
Note that this way the defender intercepts any attack with probability 2/3, which is strictly greater than $(\sqrt{5} - 1)/2$.
What is interesting about this observation is not that an additional information could help the defender, rather why this particular information would help the defender.
The reason resides in the nature of patrolling games, specifically that the defender has to think in to the future more than just $d$ steps ahead.
If she had the information about an initiated attack, she would need not to care about the future and could act in a way that would otherwise afford the attacker the opportunity to successfully attack with higher probability.
\end{example}

%TODO The Existence?
\chapter{Existence of Optimal Strategies}

In this chapter, we summarize the essential results achieved in \cite{iti1}, while we primarily focus on the constructs on which our results are based.

We start by defining and describing a stochastic game.
Then, we translate a given patrolling game $\mathcal{G}$ into a turn-based perfect-information stochastic safety game $\mathcal{S}$, where the defender corresponds to the maximizer.
We state that the maximizer has an optimal strategy in $\mathcal{S}$, which can be translated back to $\mathcal{G}$, yielding the same value of the game.
We skip some technical parts (mostly proofs) which can be found in \cite{iti1}.

%TODO jedina sekcia - nezmazat/nespravit dve?
\section{Translating Patrolling Games into Stochastic Games}

%the content, including definitions etc., is from \cite{iti1}

\begin{definition}
%$(V_\Diamond, V_\square)$ - Diamond = Min, Square = Max
A \emph{stochastic game} is a tuple $\mathcal{S} = (V, (V_{\emph{Min}}, V_{\emph{Max}}), \ell, A, \mathbf{E}, \mathbf{P})$ where $V$ is a set of \emph{vertices}, $(V_{\emph{Min}}, V_{\emph{Max}})$ is a partition of $V$, $\ell \in \mathbb{N}$, $A \subseteq \mathbb{R}^\ell$ is a set of \emph{actions}\footnote{
The reason why actions are defined as real-valued vectors of a fixed dimension might not be clear at the moment.
Such a set of actions satisfies some conditions which are used later to prove the existence of an optimal strategy for player \emph{Max}.}, $\mathbf{E} : V \to 2^A$ assigns to each vertex a set of \emph{enabled actions}, and $\mathbf{P} : V \times A \to \Delta(V)$ assigns to each $(v, a) \in V \times A$ a discrete probability distribution over $V$ with finite support.
%TODO: each $(v, a), v \in V, a \in E(v)$ 

\end{definition}
%nikde ziaden zaciatocny stav!!!

%TODO: $P(v, a)$ is relevant only for $a \in E(v)$

A stochastic game $\mathcal{S}$ is played between two players, \emph{Min} and \emph{Max}.
A play of $\mathcal{S}$ starts in some vertex $u \in V$ and then proceeds to other vertices based on the actions chosen.
Player $\bigcirc \in \{\emph{Min}, \emph{Max}\}$ chooses actions in the vertices of $V_\bigcirc$.
When the play is in a vertex $v \in V_\bigcirc$ and player $\bigcirc$ selects an action $a \in \mathbf{E}(v)$, the next node is chosen randomly according to the distribution $\mathbf{P}(v, a)$.
%Note that
Players can choose actions based on all the vertices already visited, not just the current one.
However, the players cannot randomize.\footnote{As we will see, for our purposes of translation between patrolling and stochastic games, players in stochastic games need not to randomize---for every vertex $v \in V_{\emph{Max}}$, the set of enabled actions $\mathbf{E}(v)$ will be closed under convex combination, and the attacker cannot randomize even in patrolling games.}
Formally, a \emph{strategy} for player $\bigcirc \in \{\emph{Min}, \emph{Max}\}$ is a function $\theta_\bigcirc : V^*V_\bigcirc \to A$ satisfying $\theta_\bigcirc (wv) \in \mathbf{E}(v)$ for all $w \in V^*$ and $v \in V_\bigcirc$.

Similarly to patrolling games, every initial vertex $v \in V$ and every pair of players' strategies $(\theta_{\emph{Min}}, \theta_{\emph{Max}})$ determine a unique probability space over all infinite sequences of vertices starting in $v$ (see \cite{chung}).
Given an initial vertex $v \in V$, a target vertex $t \in V$, and a pair of players' strategies $(\theta_{\emph{Min}}, \theta_{\emph{Max}})$, we denote by $val^\mathcal{S}(\theta_{\emph{Min}}, \theta_{\emph{Max}}, v, t)$ the probability of all infinite sequences of vertices starting in $v$ that \emph{never} visit $t$.
We define $val^\mathcal{S}(\theta_{\emph{\emph{Max}}}, v, t) = \inf_{\theta_{\emph{Min}}}val^\mathcal{S}(\theta_{\emph{Min}}, \theta_{\emph{Max}}, v, t)$. Furthermore, we define $val^\mathcal{S}(v, t) = \sup_{\theta_{\emph{Max}}}val^\mathcal{S}(\theta_{\emph{Max}}, v, t)$.
Given a target $t \in V$, a strategy $\theta_{\emph{Max}}^*$ is called \emph{optimal} if $val^\mathcal{S}(\theta_{\emph{Max}}^*, v, t) = val^\mathcal{S}(v, t)$ for all $v \in V$.

\

Now we present the idea behind the translation of imperfect-information patrolling games into perfect-information stochastic games.
Patrolling \linebreak games employ the Stackelberg model, where the defender first commits to a strategy and has no knowledge about the attacker, whereas the attacker has complete knowledge about the defender's strategy and her current position.
As we have seen in Example \ref{ex:triangle}, it is crucial that the defender cannot exploit the knowledge that the attacker has already attacked.
To cope with this restriction, we define the stochastic game in a way that the vertices correspond to defender's $d$-step randomized plans, where $d$ is the attack length.
The actions of player \emph{Max} correspond to selecting and prolonging these plans and the actions of player \emph{Min} correspond to attacker's decisions whether to attack some target or not.
When player \emph{Min} chooses an action corresponding to attacking a target, the outcome of the game is immediately computed based on the fixed $d$-step plan, and the players can make no more choices afterwards.
When player \emph{Min} chooses the action corresponding to attacker's waiting, player \emph{Max} has to prolong 
the previously fixed $d$-step plan by one step and the first step of the plan is performed, so it remains a $d$-step plan.\footnote{Although it would be sufficient to first perform the first step of the plan, and then prolong it by one step, we prefer to be consistent with \cite{iti1}.}
%jedine dolezite je, aby bol fixnuty d-commitment, ked ma hrac Min moznost zautocit
%zlozitost nas absolutne netrapi - SG su len kvoli dokazu existencie 
%TODO - dat inam footnote? Nerozbilo by to definiciu na 2 strany.
%the first step of the previously fixed $d$-step plan is performed and player \emph{Max} has to prolong the plan by %one step for it to remain a $d$-step plan.}

%TODO - do we need this? if yes, do we need it here?
%In \cite{iti2}, it has been shown that if the defender uses a given strategy $\sigma$, there is an attacker's best response strategy $\pi$ which attacks with probability 1.
%Therefore, (in the rest of this chapter/section) we assume that the attacker attacks with probability 1.
%(plays according to such a strategy)

%definovat $att-val$ - ^^ comes quite obvious from the observation in the paragraph where $att-val$ is defined

After informal description of the translation process, let us become more precise.
We start with formalizing the concept of a $k$-step randomized plan.
Let us fix a patrolling game $\mathcal{G} = (U, E, T, \hat{u}, d)$.

\begin{definition}
Let $k \in \mathbb{N}$. A \emph{$k$-commitment} is a function $f : U^{\leq k} \to \Delta(U)$ satisfying the following conditions:
\vspace{-5pt}
\begin{itemize}[noitemsep]
	\item $f(\epsilon)(u) = 1$ for some $u \in U$,
	\item $supp(f(wu)) \subseteq succ(u)$ for all $w \in U^{\leq k - 1}$ and $u \in U$.
\end{itemize}
\vspace{-5pt}
The \emph{root} of a commitment f, denoted by $r(f)$, is the unique node $u$ satisfying $f(\epsilon)(u) = 1$.
We denote by $\mathbf{Comm}_k$ and $\mathbf{Comm}_k(u)$ the sets of all $k$-commitments and of all $k$-commitments $f$ with $r(f) = u$, respectively.
Further, we define $\xi(f) = f(r(f))$ to be the probability distribution in the root of $f$.
\end{definition}

A given $d$-commitment $f$ can be interpreted as a Markov chain with $U^{\leq d+1}$ as the set of states and the transition probabilities given by $f$ (the states of $U^{d+1}$ are absorbing).
Once the defender is in a node $u$ and a $d$-commitment $f \in \mathbf{Comm}_d(u)$ is fixed, the probability of detecting an immediate attack on a target can be computed in the following way.
For every $d$-commitment $f$ and every target $t \in T$, we define the value $val(f, t)$ of defending target $t$ with commitment $f$ as the probability of reaching a state of the form $wt, w\neq \epsilon$
%TODO for some $w \neq \epsilon$ / where $w$ is a non-empty word
from state $\epsilon$ in the Markov chain induced by $f$.
Further, we define $val(f) = \text{min}_{t\in T} ~val(f, t)$.

The concept of extending a $d$-commitment is quite intuitive.
Given a $d$-commitment $f$ and a $d+$1-commitment $g$, we say that $g$ \emph{extends} $f$ if for every $w \in U^{\leq d}$ we have that $g(w) = f(w)$.
Further, we say that a $d$-commitment $f'$ is the \emph{$u$-suffix} of $g$ if $r(f') = u$ and for every $w \in U^{\leq d-1}$ we have that $f'(uw) = g(r(g)uw)$.
%TODO priblizit, co je to u-suffix
Player \emph{Max}'s new $d$-step plan is actually a $u$-suffix of an extension of her original plan.

%TODO delete the first sentence? merge it with the footnote ^^? change its location?
Now follows the definition of a stochastic game corresponding to the fixed patrolling game $\mathcal{G}$, as presented in \cite{iti1}.
We define a stochastic game $\mathcal{S} = (V, (V_{\emph{Min}}, V_{\emph{Max}}), |U|^{d+2}, A, \mathbf{E}, \mathbf{P})$ where players \emph{Min} and \emph{Max} correspond to the attacker and defender in $\mathcal{G}$, respectively.
\vspace{-5pt}
\begin{itemize}[noitemsep]
	\item $V = (\{\emph{Min}, \emph{Max}\} \times \mathbf{Comm}_d) \cup \{init, succ, fail\}$
	\item $V_{\emph{Min}} = (\{\emph{Min}\} \times \mathbf{Comm}_d)$
	\item $V_{\emph{Max}} = (\{\emph{Max}\} \times \mathbf{Comm}_d) \cup \{init, succ, fail\}$
	\item $A = \mathbf{Comm}_d \cup \mathbf{Comm}_{d+1} \cup T \cup \{\perp\}$
	\item the action enabling function $\mathbf{E}$ is defined as follows:
	\vspace{-4pt}
	\begin{itemize}[noitemsep]
		\item $\mathbf{E}(init) = \mathbf{Comm}_d(\hat{u}), \mathbf{E}(succ) = \mathbf{E}(fail) = \{\perp\}$ 
		\item $\mathbf{E}((\emph{Min}, f)) = T \cup \{\perp\}$ 
		\item $\mathbf{E}((\emph{Max}, f)) = \{g \in \mathbf{Comm}_{d+1} \mid g~\text{extends}~f\}$ 
	\end{itemize}
	\vspace{-4pt}
	\item the function $\mathbf{P}$ is defined as follows:
	\vspace{-4pt}
	\begin{itemize}[noitemsep]
		\item $\mathbf{P}(init, f)((\emph{Min}, f)) = 1$
		\item for $(\emph{Min}, f) \in V_{\emph{Min}}$ we put
		\vspace{-2pt}
		\begin{itemize}[noitemsep]
			\item $\mathbf{P}((\emph{Min}, f), \perp)((\emph{Max}, f)) = 1$
			\item $\mathbf{P}((\emph{Min}, f), t)(succ) = val(f, t)$
			\item $\mathbf{P}((\emph{Min}, f), t)(fail) = 1 - val(f, t)$
		\end{itemize}
		\vspace{-2pt}
		\item $\mathbf{P}((\emph{\emph{Max}}, f), g)((\emph{Min}, f')) = f(r(f))(u)$ where $f'$ is the $u$-suffix~of~$g$
		\item $\mathbf{P}(succ, \perp)(succ) = \mathbf{P}(fail, \perp)(fail) = 1$
	\end{itemize}
	\vspace{-4pt}
\end{itemize}
\vspace{-5pt}

%Now it becomes more clear why actions are defined as real-valued vectors of a fixed dimension.
It remains to show that $A$ is a subset of $[0, 1]^{|U|^{d+2}}$.
Remember that a $d$-commitment can be interpreted as a Markov chain with a tree-like structure ($|U|$-ary tree of depth $d$.)
Putting the values of a $d+$1-commitment next to each other, the commitment can be seen as a vector of dimension $|U|^{d+2}$.
Coding the $d$-commitments as such vectors is straightforward and the remaining actions, $T \cup \{\perp\}$, can be mapped to arbitrary distinct vectors of $[0, 1]^{|U|^{d+2}}$ which do not represent commitments.

The process of translating a defender's strategy $\sigma$ in $\mathcal{G}$ to a strategy $\theta_{\emph{Max}}[\sigma]$ of player \emph{Max} in $\mathcal{S}$, and vice versa ($\theta_{\emph{Max}}$ to $\sigma[\theta_{\emph{Max}}]$), is described in detail in \cite{iti1}.
There, it has also been proven that this translation preserves the values of the strategies, which is captured by the following lemma.
%i.e. $val^\mathcal{S}(\theta_{\emph{Max}}[\sigma], init, fail) = val(\sigma)$ and $val^\mathcal{S}(\theta_{\emph{Max}}, init, fail) = val(\sigma[\theta_{\emph{Max}}])$.

\begin{lemma}
\label{lem:translation}
For every defender's strategy $\sigma$ in $\mathcal{G}$ we have that $val^\mathcal{S}(\theta_{\emph{Max}}[\sigma], init,$ $fail) = val(\sigma)$.
Further, for every strategy $\theta_{\emph{Max}}$ of player \emph{Max} in $\mathcal{S}$ we have that $val^\mathcal{S}(\theta_{\emph{Max}}, init, fail) = val(\sigma[\theta_{\emph{Max}}])$.
\end{lemma}

In general, optimal strategies for player \emph{Max} in stochastic games may not exist---some extra conditions have to be satisfied.
In \cite{iti1}, it has been proven that $\mathcal{S}$ satisfies such sufficient conditions and there exists an optimal memoryless strategy for player \emph{Max} in $\mathcal{S}$.

Combining the upper two paragraphs, we get:
\begin{thm}
\label{thm:existence}
The defender has an optimal strategy in $\mathcal{G}$.
\end{thm}

For details of the translation process and the mentioned proofs, see \cite{iti1}.

\chapter{Main Result}
%TODO chapter intro
In this chapter, we review and extend the results presented in \cite{iti1}.
We provide an algorithm for computing an $\epsilon$-optimal defender's strategy.
We encode this strategy using a deterministic finite-state automaton of size ... %TODO

\section{Properties of Optimal Strategies in Patrolling Games}
\label{sec:value}
In this section, we look at what happens with a defender's strategy after a history has been played.
%Recall that $val_u(\sigma) = \inf_{\pi \in \Pi}\mathcal{P}_u^{\sigma}(\mathcal{D}_u[\pi])$.

\begin{definition}
Let $\sigma$ be a defender's strategy and $h$ a history.
We define a strategy $\sigma_h$ by $\sigma_h(last(h)h') = \sigma(hh')$ for all $h' \in N^*$.
Further, we define the value of $\sigma$ after the history $h$ has been played by $val(\sigma \mid h) = val_{last(h)}(\sigma_h)$.
\end{definition}

The following lemma from \cite{iti1} states that each optimal strategy retains its value throughout a play of the game.
This knowledge is crucial in our argumentation about $\epsilon$-optimality in the next section.

\begin{lemma}
\label{lem:value}
Let $\sigma \in \Sigma$ be an optimal defender's strategy and $h \in \mathcal{H}(\sigma)$ a history.
We have that $val(\sigma \mid h) = val(\sigma)$.
\end{lemma}
%Note that $val(\sigma \mid h \hat{u}) = val_{\hat{u}}(\sigma_{h\hat{u}}) \leq val(\sigma)$.

%TODO - proof? druha cast sa moze zist pri dokazovani pre multiple robots
%\begin{proof}
Full proof of this lemma can be found in \cite{iti1}.
Herein, we just broadly describe the structure and basic idea of the proof.
First, it is shown by contradiction that $val(\sigma \mid h) \leq val(\sigma)$ for all $h \in \mathcal{H}(\sigma)$.
Otherwise, one could iteratively construct a history $h''$ such that $val(\sigma \mid h'') > 1$.

One of the key observations used in the proof is that the value of $\sigma$ cannot decrease if we do not allow the attacker to attack in the first $k \in \mathbb{N}$ steps.
This observation is also used in the second part of the proof to show that $val(\sigma \mid h) = val(\sigma)$ for all $h \in \mathcal{H}(\sigma)$, specifically in the first inequality here:
$$val(\sigma) \leq \sum_{h' \in {\mathcal{H}(\sigma)} \wedge |h'| = k}(val(\sigma \mid h')) \cdot P^\sigma(\mathcal{R}(h'))) \leq val(\sigma)$$
%TODO skarede rozdelenie
The second inequality follows from the first part of the proof, i.e. $val(\sigma~\mid~h) \leq val(\sigma)$ for all $h \in \mathcal{H}(\sigma)$, and the fact that $\sum_{h' \in {\mathcal{H}(\sigma)} \wedge |h'| = k} P^\sigma(\mathcal{R}(h')) = 1$.
%\end{proof}

Note that Lemma \ref{lem:value} would not hold without the restriction that the initial node has to be a target (see Example \ref{ex:initial} for counterexample).

\section{Discretization and $\epsilon$-optimality}
%TODO uviest to tu

As it is common in algorithmic game theory, we are interested in strategies implementable by finite-memory controllers.
In order to formalize this concept, we define a \emph{regular} strategy.
%TODO foonote? %We prefer the definition from... for its simplicity.
%We use the definition from \cite{iti2} as we find the definition from \cite{iti1} too complex for our purposes.
%As stated in / According to \cite{iti2}
A defender’s strategy $\sigma$ is \emph{regular} if there is a deterministic finite-state automaton (DFA) $\mathcal{A}$ over the alphabet $U$ (the nodes) such that $\sigma(h)$ depends only on the control state entered by $\mathcal{A}$ after reading $h$ \cite{iti2}.
%TODO zacitovat to nejak?

In the rest of this section, we prove the existence of a regular $\epsilon$-optimal defender's strategy.
We proceed in three steps.
We start with an optimal defender's strategy $\sigma$, whose existence is guaranteed by Theorem \ref{thm:existence}.
First, we discretize $\sigma$ to obtain an $\epsilon$-optimal strategy $\sigma_\epsilon$.
Second, using translation process from the previous chapter, we translate $\sigma_\epsilon$ from $\mathcal{G}$ to a strategy $\theta_\emph{Max}$ in the corresponding stochastic game $\mathcal{S}$.
As $\theta_\emph{Max}$ stays in finitely many vertices, we restrict the set of vertices of $\mathcal{S}$ to be finite.
We apply standard results for finite-state stochastic games to obtain an optimal memoryless strategy $\theta_\emph{Max}^{ml}$.
Third, using $\theta_\emph{Max}^{ml}$, we construct a DFA which encodes an $\epsilon$-optimal strategy.

\textbf{Step 1.} Let us fix an optimal defender's strategy $\sigma$, whose existence is guaranteed by Theorem \ref{thm:existence}.
%For a desired precision $\epsilon$
We discretize $\sigma$ to obtain a strategy $\sigma_\epsilon$ with all probabilities $\sigma_\epsilon(h)(u)$ being integer multiples of an appropriate number $t$ satisfying \mbox{$t = 1/l$} for some $l \in \mathbb{N}$.
%TODO
As we will see, to guarantee the $\epsilon$-optimality of $\sigma_\epsilon$, it suffices that $|\sigma(h)(u) - \sigma_\epsilon(h)(u)| \leq {\epsilon\over{|N|d}}$.
We can guarantee this property for $t = \lceil(|N|d)/\epsilon\rceil^{-1}$ using the following rounding procedure.

Let $U = \{u_1, u_2, \dots, u_{|U|}\}$ be the set of nodes of $\mathcal{G}$.
For every history $h$ and every $1 \leq i \leq |U|$, we inductively define $\sigma_\epsilon(h)(u_i) = k_i \cdot t$, where $k_i$  is the largest number satisfying
$$ k_i \cdot t \leq \sigma(h)(u_i) + \sum_{j=1}^{i-1}(\sigma(h)(u_j) - \sigma_\epsilon(h)(u_j))~.$$
This rounding procedure guarantees %that the resulting discretized strategy $\sigma_epsilon$ is valid in the sense
that $\sigma_\epsilon(h)$ is indeed a probability distribution over $U$, i.e. $\sum_{u\in U}\sigma_\epsilon(h)(u) = 1$ (note that simple rounding would not guarantee this property).
%TODO preformulovat ?
Further, when we realize the invariant $0 \leq \sum_{j=1}^{i-1}(\sigma(h)(u_j) - \sigma_\epsilon(h)(u_j)) < t$ holds for all $1 \leq i \leq |U|$, it is easy to see that $|\sigma(h)(u_i) - \sigma_\epsilon(h)(u_i)| < t$.% (for all $i$)

We base the proof of the $\epsilon$-optimality of $\sigma_\epsilon$ on the following two observations.
First, we observe that whenever the attacker attacks, the defender will capture him using $\sigma$ with probability at least as large as $val(\sigma)$.
And second, we argue that the probability of capturing the attacker in the same moment using $\sigma_\epsilon$ is at least as large as using $\sigma$ minus $\epsilon$.

To formulate this, let us start with a formal definition of the defender's value against an immediate attack after a given history.

\begin{definition}
Let $\iota$ be a defender's strategy and $h$ a history. We define
$$att\text{-}val(\iota \mid h) = \inf_{t \in T} \mathcal{P}_{last(h)}^{\iota_h}(D_t)$$
where $D_t$ is the set of all runs that visit $t$ in the first $d$ steps.
\end{definition}

%we prove even stronger proposition
\begin{lemma}
\label{lem:epsilon}
%Let $h \in \mathcal{H}(\sigma_\epsilon)$ be a history.
For every $h \in \mathcal{H}(\sigma_\epsilon)$ we have that $att\text{-}val(\sigma_\epsilon \mid h) \geq val(\sigma) - \epsilon$.
%$\inf_{h \in \mathcal{H}(\sigma_\epsilon)} att\text{-}val(\sigma_\epsilon \mid h) \geq val(\sigma) - \epsilon$
\end{lemma}

\begin{proof}
\end{proof}

%TODO zmazat
\begin{proposition}
$\sigma_\epsilon$ is an $\epsilon$-optimal strategy.
\end{proposition}

%TODO zmazat
\begin{proof}
\end{proof}

\textbf{Step 2.}
According to Lemma \ref{lem:translation}, we can translate the strategy $\sigma_\epsilon$ in $\mathcal{G}$ to a strategy $\theta_{\emph{Max}} := \theta_{\emph{Max}}[\sigma_\epsilon]$ in $\mathcal{S}$ such that
$$val^\mathcal{S}(\theta_\emph{Max}, init, fail) = val(\sigma_\epsilon)~.$$
Let us denote by $\mathbf{Reach}(\theta_\emph{Max})$ the set of all vertices in $\mathcal{S}$ that are reachable from $init$ using $\theta_\emph{Max}$.
%for every 1, every 2 and every 3 (each/all)
Note that for every $(\bigcirc, f) \in \mathbf{Reach}(\theta_\emph{Max})$, $w \in U^{\leq d}$ and $u \in U$ we have that $f(w)(u)$ is an integer multiple of $t$.
As there are only finitely many such commitments $f$, the set $\mathbf{Reach}(\theta_\emph{Max})$ is finite.

By restricting the set of vertices of $\mathcal{S}$ to $\mathbf{Reach}(\theta_\emph{Max})$, we obtain a stochastic game $\mathcal{S}_\epsilon$.
As $\theta_{\emph{Max}}$ is a valid strategy in $\mathcal{S}_\epsilon$, we have
$$val^{\mathcal{S}_\epsilon}(\theta_\emph{Max}, init, fail) = val^\mathcal{S}(\theta_\emph{Max}, init, fail)~.$$
Applying results for perfect-information stochastic games with finitely many states (see \cite{kucera}), we obtain an optimal memoryless\footnote{A \emph{memoryless} strategy depends only on the current vertice. Therefore, a memoryless strategy for player \emph{Max} can be defined as a function $\theta_\emph{Max}^{ml} : V_\emph{Max} \to A$.} strategy $\theta_\emph{Max}^{ml}$ such that
$$val^{\mathcal{S}_\epsilon}(\theta_\emph{Max}^{ml}, init, fail) \geq val^{\mathcal{S}_\epsilon}(\theta_\emph{Max}, init, fail)~.$$
As $\mathbf{Reach}(\theta_\emph{Max})$ is a subset of vertices of $\mathcal{S}$, $\theta_\emph{Max}^{ml}$ is a valid strategy in $\mathcal{S}$ and we have
$$val^\mathcal{S}(\theta_\emph{Max}^{ml}, init, fail) = val^{\mathcal{S}_\epsilon}(\theta_\emph{Max}^{ml}, init, fail)~.$$
Summing up, $\theta_\emph{Max}^{ml}$ is a memoryless strategy %in $\mathcal{S}$
satisfying
$$val^\mathcal{S}(\theta_\emph{Max}^{ml}, init, fail) \geq val(\sigma_\epsilon)~.$$

\textbf{Step 3.}
%TODO %IDEA $q_0$ spravit ako "nelegalny" commitment - stav musi ale splnat vsetko potrebne!
Using $\theta_\emph{Max}^{ml}$, we construct a DFA $\mathcal{A}$ encoding the desired regular \hbox{$\epsilon$-optimal} strategy $\sigma_\epsilon^{reg}$ as follows: $\mathcal{A} = (Q_0, U, \delta, q_0, \{\})$ where
\vspace{-3pt}
\begin{itemize}[noitemsep]
	\item $Q_0 = Q \cup \{q_0\}$ is the finite set of states, where $Q = \{f \mid (\emph{Max}, f) \in \mathbf{Reach}(\theta_\emph{Max})\}$
	\item $U$ is the alphabet, %a finite set of input symbols,
	\item $\delta : Q_0 \times U \to Q$ is a transition function such that
%defined as follows: + spravit "rozdvojku"
$\delta(q_0, \hat{u}) = \theta_\emph{Max}^{ml}(init)$ and
$\delta(f, u) = f'$ where $f'$ is the $u$-suffix of $\theta_\emph{Max}^{ml}((\emph{Max}, f))$,\footnote{Note that $\theta_\emph{Max}^{ml}$ takes as an argument only a single vertex. That would not be possible unless $\theta_\emph{Max}^{ml}$ was a memoryless strategy.} %IDEA footnote napisat niekam inam? (ako remark?)
	\item $q_0$ is the initial state.\footnote{The initial state ${q_0}$ is just an auxiliary state for reading $h_0$, which represents no information as it is always $\hat{u}$.}
\end{itemize}
\vspace{-3pt}
We denote by $\mathcal{A}[h]$ the state of $\mathcal{A}$ reached after reading the history $h$.
%TODO dat definiciu $\xi(f)$ sem? nikde sa doteraz nepouzila
We define $\sigma_\epsilon^{reg}(h) = \xi(\mathcal{A}[h])$ (recall that $\xi(f) = f(r(f))$ for a commitment $f$).
It is easy to show that $\sigma_\epsilon^{reg}$ is the same as $\sigma[\theta_\emph{Max}^{ml}]$ obtained from $\theta_\emph{Max}^{ml}$ using Lemma \ref{lem:translation}.
It follows that
$$val(\sigma_\epsilon^{reg}) = val^\mathcal{S}(\theta_\emph{Max}^{ml}, init, fail) \geq val(\sigma_\epsilon) \geq val(\sigma) - \epsilon~,$$
i.e. $\sigma_\epsilon^{reg}$ is $\epsilon$-optimal.

%%%%%%%%%%%
%COMMENT OUT
%The following observation follows from Lemma \ref{lem:epsilon} and the properties of the translation process from $\mathcal{G}$ to $\mathcal{S}$.

%Realizing that $Q$ is the set of all $d$-commitments $f$ occuring in vertices of $\mathbf{Reach}(\theta_\emph{Max})$:
%TODO treba mi to?
%\begin{lemma}
%\label{lem:epsilon_commitment}
%%All $d$-commitments $f$ occuring in vertices of $\mathbf{Reach}(\theta_\emph{Max})$ satisfy
%For every $d$-commitment $f \in Q$, we have that $val(f) \geq val(\sigma) - \epsilon$.
%\end{lemma}

%%TODO pozmenit to tu cele - vratane predchadzajucich Lemmatov
%From the definitions of $\delta$ and $\sigma_\epsilon^{reg}$ it follows that
%%Lemma \ref{lem:epsilon} also holds for $\sigma_\epsilon^{reg}$
%$att\text{-}val(\sigma_\epsilon^{reg} \mid h) = val(\mathcal{A}[h])$.
%%summarized in the following:
%\begin{corollary}
%\label{cor:epsilon_reg}
%For every $h \in \mathcal{H}(\sigma_\epsilon^{reg})$ we have $att\text{-}val(\sigma_\epsilon^{reg} \mid h) \geq val(\sigma) - \epsilon$.
%\end{corollary}
%%%%%%%%%%%

%TODO zmenit nazov?
%\section{Reducing the State Space}
\section{Characteristics}

As you may noticed, the above proof of existence of an $\epsilon$-optimal strategy is not constructive.
In \cite{iti1}, the authors provide an algorithm which computes the desired strategy.
They introduce the concept of a \emph{discretized} $d$-commitment $f$ for which it holds that for all $w \in U^{\leq d}$ and $u \in U$ the number $f(w)(u)$ is an integer multiple of $t$.
Their algorithm is based on searching for a \emph{closed} set of such commitments (the defender stays within this set by prolonging her plans) that maximizes the minimal value of all commitments.
The algorithm runs in time polynomial in the number of all discretized $d$-commitments, i.e. in time ${\left({{|N|d}\over \epsilon}\right)}^{|N|^{\mathcal{O}(d)}}$.

%TODO zmienit sa o nasom algoritme / jeho zlozitosti?
To reduce the number of possible states of the automaton $\mathcal{A}$ encoding the $\epsilon$-optimal strategy, we define an equivalence relation on the states of $\mathcal{A}$ which we use to factorize the state-space of $\mathcal{A}$.
The equivalence relation is based on the ability to defend an attack which started somewhen in the last $d$ steps. % $d-1$ steps
We call this ability the \emph{characteristics} of a given state.
We prove that states with the same characteristics are equivalent, i.e. they can be merged into a single state without influencing the value of the resulting strategy.

In the following definition, we assume the automaton $\mathcal{A}$ constructed in Step 3 in the previous section.

\begin{definition}
%TODO Let $\mathcal{A}$ be the automaton constructed in Step 3 in the previous section.
Given $p \in Q$, we define the characteristics $\mathbf{Char}_p$ of $p$ to be a function which to every pair $(k, u) \in \{0, \dots, d\} \times U$ assigns the probability of reaching $u$ in at most $k$ steps whenever the strategy is in the state $p$ (recall that $p$ determines the current state of the play).
%We omit the superscript $\mathcal{A}$ in $\mathbf{Char}_p^{\mathcal{A}}$ when it is clear from the context.
\end{definition}
%Values for $k = 0$ contain the information about the current location of the defender and 
%values for $k = 1$ represent the probability distribution over the nodes in the next step.
The current location of the defender is determined by the only node $u$ for which $\mathbf{Char}_p(0, u) = 1$ and the probability distribution over the nodes in the next step is represented by values of $\mathbf{Char}_p(1, u)$.

\

Given $q \in Q$, we denote by $\mathcal{A}_{p \leftarrow q} = (Q_0', U, \delta', q_0, \{\})$ the DFA defined as follows:
\vspace{-5pt}
\begin{itemize}[noitemsep]
	\item $Q_0' = Q_0 \smallsetminus \{q\}$
	\item $\delta'(r, u) =
  \begin{cases}
   p		& \text{if } \delta(r, u) = q \\
   \delta(r, u)	& \text{otherwise.}
  \end{cases}$
\end{itemize}
\vspace{-5pt}

Intuitively, $\mathcal{A}_{p \leftarrow q}$ is obtained from $\mathcal{A}$ by redirecting all transitions going to $q$ to $p$ and then removing $q$.

This change in the transition function might influence the characteristics of the states from which $q$ was reachable in less than $d$ steps.
%To be able to distinguish between the old and new characteristics of a state $r$,
For $r \in Q$, we denote by $\mathbf{Char}'_r$ the characteristics of $r$ in the automaton $\mathcal{A}_{p \leftarrow q}$.

%IDEA - The following lemma states that ...
\begin{lemma}
\label{lem:characteristics}
Given $p, q \in Q$ such that $\mathbf{Char}_p = \mathbf{Char}_q$, we have that for all $r \in Q \smallsetminus \{q\}$
$$\mathbf{Char}'_r = \mathbf{Char}_r~.$$
\end{lemma}

\begin{proof}
Observe that the automaton $\mathcal{A}$ together with the strategy it determines can be seen as a Markov Chain.
%TODO premenovat $f$ a $g$ na $r$ a $s$
Indeed, when the play is in a state $f$, the next state $g$ is chosen with probability $\sum_{u \in U, g = \delta(f, u)} \xi(f)(u)$.
%TODO remove the reasoning
This is because the next node $u$ visited by the defender is selected based on $\xi(f)$, and $g$ is determined by $\delta(f, u)$.

That means that although the states of $\mathcal{A}$ are $d$-commitments, only the probability distribution in their root affects the strategy. %(the rest of the commitment just distinguishes it from other states)
%We do not know whether, in general, the state-space can be bounded by the number of possible probability distributions in a root, but we strongly believe it cannot. %dost by to ale pomohlo, lebo najmensia mozna pravdepodobnost v charakteristikach by nebola $t^d$, ale "len" $t$
The idea of the proof is that the characteristics of a state $r$ are completely determined by
(1) the probability distribution in its root and by
%, by the transitions from it,
(2) the characteristics of all the states to which there is a transition from $r$ under particular node~$u$.
\
%TODO dat to ako odrazky?
%\vspace{-20pt}
%\begin{itemize}[noitemsep, label=(1)]
%	\item the probability distribution in its root
%\end{itemize}
%\vspace{-35pt}
%\begin{itemize}[noitemsep, label=(2)]
%	\item the characteristics of all the states to which there is a transition from $r$ under a particular node $u$.
%\end{itemize}
%\vspace{-20pt}
\
Substitution of $q$ by $p$ does not have any influence on (1) and because $\mathbf{Char}_p = \mathbf{Char}_q$, neither has it any influence on (2).
%Note that the characteristics of a state preserve the information about probability distribution in its root.

%IDEA change the order of $r, v, k$
Formally, we prove by induction on $k$ that for all $r \in Q \smallsetminus \{q\}$, $v \in U$, $0 \leq k \leq d$ we have that $$\mathbf{Char}'_r(k, v) = \mathbf{Char}_r(k, v)~.$$
For $k = 0$ this clearly holds as the root of any state remains unchanged.

Now all we need is to realize that $\xi(r)$ is the same in both $\mathcal{A}$ and $\mathcal{A}_{p \leftarrow q}$.
For $k = 1$ we have that $$\mathbf{Char}'_r(1, v) = \xi(r)(v) = \mathbf{Char}_r(1, v)~.$$

For $2 \leq k \leq d$ we have
\begin{align}
\mathbf{Char}'_r(k, v)
  &= \xi(r)(v) + \sum_{u \neq v}\xi(r)(u) \cdot \mathbf{Char}'_{\delta'(r, u)}(k - 1, v)\label{eq:first} \\
  &= \xi(r)(v) + \sum_{u \neq v}\xi(r)(u) \cdot \mathbf{Char}_{\delta'(r, u)}(k - 1, v)\label{eq:second} \\
  &= \xi(r)(v) + \sum_{u \neq v}\xi(r)(u) \cdot \mathbf{Char}_{\delta(r, u)}(k - 1, v)\label{eq:third} \\
  &= \mathbf{Char}_r(k, v)\label{eq:fourth}~.
\end{align}
\vspace{-20pt}
\begin{itemize}[noitemsep, label=--]
	\item The equalities \eqref{eq:first} and \eqref{eq:fourth} follow directly from the definition of the characteristics. %and the fact that $\mathbf{Char}_{\delta(r, u)}(0, v) = 0$ for all $u \neq v$.
	\item The equality \eqref{eq:second} holds by the induction hypothesis.
	\item To prove the equality \eqref{eq:third}, note that the only case when $\delta'(r, u) \neq \delta(r, u)$ is when $\delta'(r, u) = p$ and $\delta(r, u) = q$. However, $\mathbf{Char}_p = \mathbf{Char}_q$.
\end{itemize}
\vspace{-5pt}
\end{proof}
%\begin{remark}
%\textsl{Remark 1.~}
%Note that the case $k = 1$ in the proof is just a special case of the latter case as $\mathbf{Char}_{\delta(r, u)}(0, v) = 0$ for all $u \neq v$.
%\end{remark}

Lemma \ref{lem:characteristics} shows that removal of one of the "equivalent" states does not influence the characteristics of the other states.
Of great importance is that the lemma holds also for the reduced automaton $\mathcal{A}_{p \leftarrow q}$.
Indeed, the proof is based only on general properties of the characteristics and the definition of $\mathcal{A}_{p \leftarrow q}$.
%This justifies us to
This means we can repeat the process of removing equivalent states until all the states have mutually different, yet still unchanged characteristics.

\

For every state $p \in Q$, we define the value of its characteristics by $val(\mathbf{Char}_p) = \min_{t \in T} \mathbf{Char}_p(d, t)$.
Note that for $p = \mathcal{A}[h]$ for some history $h$, %and $\iota$ is the strategy determined by $\mathcal{A}$
we have that $val(\mathbf{Char}_p) = att\text{-}val(\sigma_\epsilon^{reg} \mid h)$.
In other words, if the attacker attacks when the play is in a state $p$, he is caught with probability $val(\mathbf{Char}_p)$.

%crucial that if there is a transition from $c_1$ to $c_2$ under $u$, then $c_2(0, u) = 1$.

%%%%%%%%%%%
%COMMENT OUT
%TODO zmazat
%\begin{proof}
%%na dokaz by nam malo stacit \ref{lem:epsilon} a \ref{lem:epsilon_commitment} - to prve plati aj pre pozmenenu (to dokazujem) a to druhe plati "vseobecne" (netyka sa strategie) -> cize mozem iterovat
%Given a defender's strategy $\iota$ and a history $h$ we denote by $\mathbf{Char}_h^\iota$ the function which to every $(k, u)
%TODO {1, ..., d}
%\in \{0, \dots, d\} \times U$ assigns the probability of reaching $u$ in at most $k$ steps using the strategy $\iota_h$ and starting in $last(h)$.
%%Note that for $\iota$ being equal to $\sigma_\epsilon^{reg}$ (determined by $\mathcal{A}$) we have that $\mathbf{Char}_h^\iota((k, u)) = \mathbf{Char}_p((k, u))$ for $p = \mathcal{A}[h]$.
%Let $\iota$ be equal to $\sigma_\epsilon^{reg}$ (determined by $\mathcal{A}$) and $\iota_{p \leftarrow q}$ be the same as $\sigma_{p \leftarrow q}$. %(determined by $\mathcal{A}_{p \leftarrow q}$).
%%Note that $\mathbf{Char}_h^\iota((k, u)) = \mathbf{Char}_p((k, u))$ for $p = \mathcal{A}[h]$.
%Given $p \in Q$, we denote by $\mathcal{A}_p$ the DFA obtained from $\mathcal{A}$ by changing the initial state to $p$, and by $\iota_p$ the regular strategy determined by $\mathcal{A}_p$.
%Note that for $p = \mathcal{A}[h]$, we have that $\iota_p = \iota_h$, therefore $\mathbf{Char}_h^\iota((k, u)) = \mathbf{Char}_p((k, u))$.

%We define a sequence of strategies $\sigma_0$, $\sigma_1$, $\sigma_2$, $\dots$ as follows:
%\vspace{-5pt}
%\begin{itemize}[noitemsep]
%	\item for every history $h$ where $|h| \leq i$ we have that $\sigma_i(h) = \iota_{p \leftarrow q}(h)$
%	\item for every history $hh'$ where $|h| = i$ and $|h'| \geq 1$ we have that $\sigma_i(hh') = \iota_r(h')$ where %$r = \mathcal{A}_{p \leftarrow q}[w]$.
%\end{itemize}
%\vspace{-5pt}
%Intuitively, $\sigma_i$ is determined by $\mathcal{A}_{p \leftarrow q}$ in the first $i$ steps.
%Then, $\sigma_i$ is determined by $\mathcal{A}_r$ where $r$ is the state reached by $\mathcal{A}_{p \leftarrow q}$ after reading the first $i$ steps of the history.
%
%For each $i \geq 0$, we prove that for all histories $h$ we have 
%$$att\text{-}val(\sigma_i | h) \geq val(\sigma) - \epsilon~.$$

%For $i = 0$ this follows from Lemma \ref{lem:epsilon_commitment}.
%\end{proof}
%%%%%%%%%%%


\chapter{multiple robots}
\label{chap:multiPatrol}

%ziadny zaciatocny stav - sucastou strategie je pociatocne rozlozenie - nemusi to vsak nutne byt distribucia! staci ista konfiguracia s pravdepodobnostou 1, pretoze utocnik moze zautocit az potom, t.j. nemoze zneuzit, ze vie, kde bude obranca zacinat

%vsetci sa hybu rovnako rychlo


- co sa tyka 3.1, treba povedat, ze i tu existuje optimalna strategia - t.j. compact and continuous

\chapter{This is the end, my only friend, the end}




\def\us{\char`\_}

\bibliographystyle{plain}
%\bibliography{mujbisoubor}
\begin{thebibliography}{9}

\bibitem{perimeter} N. Agmon, S. Kraus, and G. A. Kaminka. Multi-Robot Perimeter Patrol in Adversarial Settings. In \emph{ICRA}, pages 2339–2345, 2008.

\bibitem{perimeter2} N. Agmon, V. Sadov, S. Kraus, and G. A. Kaminka. The Impact of Adversarial Knowledge on Adversarial Planning in Perimeter Patrol. In \emph{AAMAS}, pages 55-62, 2008.

\bibitem{perimeter3} N. Agmon, S. Kraus, G. A. Kaminka, and V. Sadov. Adversarial Uncertainty in Multi-Robot Patrol. In \emph{IJCAI}, pages 1811-1817, 2009.

\bibitem{arbitrary} N. Basilico, N. Gatti, and F. Amigoni. Leader-Follower Strategies for Robotic Patrolling in Environments with Arbitrary Topologies. In \emph{AAMAS}, pages 57-64, 2009.

\bibitem{powerlessIntruder} N. Basilico, N. Gatti, T. Rossi, S. Ceppi, and F. Amigoni. Extending Algorithms for Mobile Robot Patrolling in the Presence of Adversaries to More Realistic Settings. In \emph{WI-IAT}, pages 557-564, 2009.

\bibitem{multi-robot} N. Basilico, N. Gatti, and F. Villa. Asynchronous Multi-Robot Patrolling against Intrusion in Arbitrary Topologies. In \emph{AAAI}, pages 1224-1229, 2010.

\bibitem{movingTargets} B. Bošanský, V. Lisý, M. Jakob, and M. Pìchouèek. Computing Time-Dependent Policies for Patrolling Games with Mobile Targets. In \emph{AAMAS}, pages 989-996, 2011.

\bibitem{line} F. Fang, A. X. Jiang, and M. Tambe. Optimal Patrol Strategy for Protecting Moving Targets with Multiple Mobile Resources. In \emph{AAMAS}, pages 957-964, 2013.

\bibitem{miso} M. Abaffy. Patrolling Games on Graphs [online]. 2013 [cit. 2014-05-10] ??? Diploma thesis. Masaryk University, Faculty of Informatics. Available at $<$http://is.muni.cz/th/321758/fi\texttt{\us}m/$>$.
%TODO

\bibitem{iti1} M. Abaffy, T. Brázdil, V. Øehák, B. Bošanský, A. Kuèera, and J. Krèál. Solving Adversarial Patrolling Games with Bounded Error. Online: ???
%TODO

\bibitem{iti2} T. Brázdil, P. Hlinìný, A. Kuèera, and V. Øehák. Optimal Strategies in Adversarial Patrolling Games. Online: ???
%TODO

\bibitem{fudenberg} D. Fudenberg and J. Tirole. Game Theory. Cambridge, MA: \emph{MIT Press}, 1991.

\bibitem{polak} L. Polák. Teorie her [Lecture notes]. Masaryk University, Brno.

\bibitem{gameTheory} Game Theory [online]. [cit. 2014-05-23]. Available at
%\url{http://en.wikipedia.org/w/index.php?title=Game_theory&oldid=610438321}
% http://en.wikipedia.org/wiki/Game_theory
%$<$http://en.wikipedia.org/w/index.php?title=Game\texttt{\us}theory\&oldid=610438321$>$.
$<$http://en.wikipedia.org/wiki/Game\texttt{\us}theory$>$.

\bibitem{nash} J. F. Nash. Equilibrium Points in $n$-Person Games. In \emph{PNAS}, volume 36, pages 48–49, 1950.

\bibitem{convex} B. von Stengel and S. Zamir. Leadership Games with Convex Strategy Sets. \emph{Games and Economic Behavior}, volume 69, pages 446–457, 2010.

\bibitem{nashStack} J. Letchford and V. Conitzer. Computing Optimal Strategies to Commit to in Extensive-Form Games. In \emph{EC}, pages 83–92, 2010.

\bibitem{interchangability} Z. Yin, D. Korzhyk, C. Kiekintveld, V. Conitzer, and M. Tambe. Stackelberg vs. Nash in Security Games: Interchangeability, Equivalence, and Uniqueness. In \emph{AAMAS}, pages 1139–1146, 2010.

\bibitem{limited1} J. Pita, M. Jain, M. Tambe, F. Ordó$\rm{\tilde{n}}$ez, and S. Kraus. Robust Solutions to Stackelberg Games: Addressing Bounded Rationality and Limited Observations in Human Cognition. \emph{Artificial Intelligence}, volume 174, pages 1142-1171, 2010.

\bibitem{limited2} B. An, D. Kempe, C. Kiekintveld, E. Shieh, S. Singh, M. Tambe, and Y. Vorobeychik. Security Games with Limited Surveillance. In \emph{AAAI}, pages 1241–1248, 2012.

\bibitem{limited3} B. An, M. Brown, Y. Vorobeychik, and M. Tambe. Security Games with Surveillance Cost and Optimal Timing of Attack Execution. In \emph{AAMAS}, pages 223–230, 2013.

\bibitem{limited4} Y. Zhang and X. Luo. Security Games with Partial Surveillance. In \emph{AAMAS}, pages 1527-1528, 2014.

\bibitem{quantal} R. D. McKelvey and T. R. Palfrey. Quantal Response Equilibria for Normal Form Games. \emph{Games and Economic Behavior}, volume 10, pages 6–38, 1995.

\bibitem{armor} J. Pita, M. Jain, J. Marecki, F. Ordó$\rm{\tilde{n}}$ez, C. Portway, M. Tambe, C. Western, P. Paruchuri, and S. Kraus. Deployed ARMOR Protection: The Application of a Game Theoretic Model for Security at the Los Angeles International Airport. In \emph{AAMAS}, pages 125–132, 2008.

\bibitem{iris} J. Tsai, S. Rathi, C. Kiekintveld, F. Ordó$\rm{\tilde{n}}$ez, and M. Tambe. IRIS - A Tool for Strategic Security Allocation in Transportation Networks. In \emph{AAMAS}, pages 37-44, 2009.

\bibitem{guards} J. Pita, M. Tambe, C. Kiekintveld, S. Cullen, and E. Steigerwald. GUARDS - Game Theoretic Security Allocation on a National Scale. In \emph{AAMAS}, pages 37-44, 2011.

\bibitem{protect} E. Shieh, B. An, R. Yang, M. Tambe, C. Baldwin, J. DiRenzo, B. Maule, and G. Meyer. PROTECT: A Deployed Game Theoretic System to Protect the Ports of the United States. In \emph{AAMAS}, pages 13-20, 2012.

\bibitem{protect_coastguard} [cit. 2014-06-01] http://teamcore.usc.edu/projects/coastguard/
%TODO - maybe delete - no citation needed

\bibitem{trusts} Z. Yin, A. X. Jiang, M. P. Johnson, M. Tambe, C. Kiekintveld, K. Leyton-Brown, T. Sandholm, and J. P. Sullivan. TRUSTS: Scheduling Randomized Patrols for Fare Inspection in Transit Systems. In \emph{IAAI}, 2012.

\bibitem{trusts2} F. M. Delle Fave, M. Brown, C. Zhang, E. Shieh, A. X. Jiang, H. Rosoff, M. Tambe, and J. P. Sullivan. Security Games in the Field: an Initial Study on a Transit System. In \emph{AAMAS}, pages 1363-1364, 2014.

\bibitem{chung} K. L. Chung. Markov Chains with Stationary Transition Probabilities. Springer, 1967.

\bibitem{kucera} A. Kuèera. Turn-based stochastic games. In \emph{K. R. Apt and E. Grädel. Lectures in Game Theory for Computer Scientists}, pages 146-184, 2011.

\end{thebibliography}


\appendix
\chapter{Ugly Proofs}



\end{document}